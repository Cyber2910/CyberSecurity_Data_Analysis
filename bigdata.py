# -*- coding: utf-8 -*-
"""BigData.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DlxqUYaOvs-j1KgYnvkEcP_RyopOkSCC
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
from numpy import loadtxt

import tensorflow.keras
from tensorflow.keras import optimizers #to optimize wieghts
from tensorflow.keras import backend as k
from tensorflow.keras.models import Sequential #sequential layers
from tensorflow.keras.layers import Activation #limit of data we used sigmoid for limiting between 0 and 1
from tensorflow.keras.layers import  Dense
import matplotlib.pyplot as plt #plotting graph
# %matplotlib inline

import pandas as pd #to work with CSV files

#Building the neural network
model = Sequential([
                    Dense(40, input_shape=(41,), activation='sigmoid'),
                    tensorflow.keras.layers.Dropout(0.2),
                    Dense(20, activation='sigmoid'),
                    tensorflow.keras.layers.Dropout(0.2),
                    
                    Dense(1, activation='sigmoid')
])
#used sigmoid because tanh gave me negative numbers which I dont want

#optimizers.RMSprop(learning_rate=0.001, rho=0.9) weight optimizer
#loss='mean_absolute_error' => Predicted value - True value to edit the weights
model.compile(optimizer=optimizers.RMSprop(learning_rate=0.001, rho=0.9),loss='mean_absolute_error',metrics=['accuracy'])

dataset = pd.read_csv('/content/Numberedtraining-set.csv') #Reading CSV
# split into input (X) and output (y) variables
X = dataset.iloc[:,1:42].values #Read all rows and columns from 1 to 41 for the features
y = dataset.iloc[:,43:44].values #Read all rows and column 43 only for the label

from sklearn.preprocessing import MinMaxScaler 
sc = MinMaxScaler(feature_range = (0, 1)) #for scaling data between 0 and 1 so that the loss wont't be so high
X_set_scaled = sc.fit_transform(X)
y_set_scaled = sc.fit_transform(y)

x_train, y_train = np.array(X_set_scaled), np.array(y_set_scaled) # transformin arrays to numpy matrices

hist = model.fit(x_train,y_train,validation_split=0.2, batch_size=82, epochs=50, shuffle=True, verbose=2) #Training the Neural Network

dataset = pd.read_csv('/content/Numberedtesting-set.csv') #Reading data for testing

y = dataset.iloc[47900:48000,43:44].values #Label


X = dataset.iloc[47900:48000,1:42].values # Features

y_arr = [] #for later visual organization
for i in y:
  for j in i:
    y_arr.append(j)


predictions = model.predict_classes(X) #predict the tested data

#inversing transformation of data but not mandatory here because we already have labels only between 0 and 1
predictions = sc.inverse_transform(predictions) 

p_arr = [] #for later visual organization
for i in predictions:
  for j in i:
    
    p_arr.append(j)

print(len(p_arr))
prd = [[]]

#organizing data in 2 dimensional array for visualizing it
prd = [y_arr,p_arr]

#for i in range(len(prd[0])):
 # print(prd[0][i],prd[1][i])

#Visualize data
plt.style.use('fivethirtyeight')
plt.plot((y_arr),label='True labels')
plt.plot(p_arr,label='Predicted Labels')
plt.xlabel('epochs')
plt.ylabel('acc %')


plt.title('True vs Predicted  Graph')
plt.legend()
plt.grid(True)
plt.show()

plt.style.use('fivethirtyeight')
plt.plot((hist.history["accuracy"]),label='y accuracy')
plt.plot(hist.history["loss"],label='loss')
plt.xlabel('epochs')
plt.ylabel('acc %')


plt.title('Accuracy vs Loss  Graph')
plt.legend()
plt.grid(True)
plt.show()

